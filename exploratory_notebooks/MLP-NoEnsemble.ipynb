{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data and select features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielwu/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2850: DtypeWarning: Columns (471,473,474,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,569,570,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,599,601,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,624,625,626,627,628,629,630,631,632,633,634,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,745,746,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,770,771,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,794,795,797,798,799,800,801,802,803,804,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SVM for Diagnosis and SVR for ADAS and Ventricles\n"
     ]
    }
   ],
   "source": [
    "from preprocess import preprocessData\n",
    "[Dtrainmat_Diagnosis, Y_FutureDiagnosis, RID] = preprocessData()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Flatten,BatchNormalization, PReLU, Activation, Lambda, ELU, MaxPooling2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.constraints import maxnorm\n",
    "import keras.utils\n",
    "\n",
    "Y = Y_FutureDiagnosis.as_matrix()\n",
    "\n",
    "\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(500, activation='relu',input_shape=(18,)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(300, activation='relu', init=\"normal\", kernel_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.3))\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(500,input_shape=(18,), kernel_initializer = 'he_normal'))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(300, kernel_initializer=\"he_normal\", kernel_constraint=maxnorm(3)))\n",
    "#     model.add(PReLU())\n",
    "#     model.add(Dropout(0.2))\n",
    "    #     model.add(Embedding(input_dim=18, output_dim=128))\n",
    "    #     model.add(LSTM(units=128))\n",
    "    model.add(Dense(3, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "    model.compile(optimizer='Adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_network(xtrain,ytrain, model=None, epochs=25, batch=64):\n",
    "    start_time = time.time()\n",
    "\n",
    "    one_hot_labels = keras.utils.to_categorical(ytrain - 1, num_classes=3)    \n",
    "    print ('Training model...')\n",
    "    model.fit(xtrain, one_hot_labels, epochs=40, batch_size=64, verbose=2)\n",
    "\n",
    "    print (\"Training duration : {0}\".format(time.time() - start_time))\n",
    "    #score = model.evaluate(X_test, y_test, batch_size=16)\n",
    "\n",
    "    #print (\"Network's test score [loss, accuracy]: {0}\".format(score))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielwu/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(300, activation=\"relu\", kernel_constraint=<keras.con..., kernel_initializer=\"normal\")`\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "6843/6843 [==============================] - 1s - loss: 0.7352 - acc: 0.6333     \n",
      "Epoch 2/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.6758 - acc: 0.6735     \n",
      "Epoch 3/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.6585 - acc: 0.6940     \n",
      "Epoch 4/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.6380 - acc: 0.6988     \n",
      "Epoch 5/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.6261 - acc: 0.7105     \n",
      "Epoch 6/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.6095 - acc: 0.7159     \n",
      "Epoch 7/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5918 - acc: 0.7278     \n",
      "Epoch 8/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5858 - acc: 0.7311     \n",
      "Epoch 9/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5777 - acc: 0.7403     \n",
      "Epoch 10/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5719 - acc: 0.7421     \n",
      "Epoch 11/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5634 - acc: 0.7456     \n",
      "Epoch 12/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5544 - acc: 0.7454     \n",
      "Epoch 13/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5426 - acc: 0.7530     \n",
      "Epoch 14/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5371 - acc: 0.7580     \n",
      "Epoch 15/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5374 - acc: 0.7529     \n",
      "Epoch 16/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5240 - acc: 0.7574     \n",
      "Epoch 17/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5115 - acc: 0.7672     \n",
      "Epoch 18/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.5050 - acc: 0.7713     \n",
      "Epoch 19/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4961 - acc: 0.7786     \n",
      "Epoch 20/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4961 - acc: 0.7719     \n",
      "Epoch 21/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4865 - acc: 0.7826     \n",
      "Epoch 22/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4814 - acc: 0.7785     \n",
      "Epoch 23/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4713 - acc: 0.7842     \n",
      "Epoch 24/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4692 - acc: 0.7871     \n",
      "Epoch 25/25\n",
      "6843/6843 [==============================] - 0s - loss: 0.4638 - acc: 0.7869     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c30f60438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.utils\n",
    "# Train all the models\n",
    "one_hot_labels = keras.utils.to_categorical(Y_FutureDiagnosis - 1, num_classes=3)    \n",
    "model = Sequential()\n",
    "model.add(Dense(500, activation='relu',input_shape=(18,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(300, activation='relu', init=\"normal\", kernel_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.3))\n",
    "#     model.add(Embedding(input_dim=18, output_dim=128))\n",
    "#     model.add(LSTM(units=128))\n",
    "model.add(Dense(3, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#tensorboard = TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=True, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None)\n",
    "model.fit(Dtrainmat_Diagnosis, one_hot_labels, epochs=25, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data and select features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danielwu/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (471,473,474,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,569,570,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,599,601,606,607,608,609,610,611,612,613,614,615,616,617,618,619,620,621,624,625,626,627,628,629,630,631,632,633,634,636,637,638,639,640,641,642,643,644,645,646,647,648,649,650,651,652,653,654,655,656,657,658,659,660,661,663,664,665,666,667,668,669,670,671,672,673,674,675,676,677,678,679,680,681,682,683,684,685,686,687,688,689,690,691,692,693,694,695,696,697,698,699,700,701,702,703,704,705,706,707,708,709,710,711,712,713,714,715,716,717,718,719,720,721,722,723,724,725,726,727,728,729,730,731,732,733,734,735,736,737,738,739,745,746,748,749,750,751,752,753,754,755,756,757,758,759,760,761,762,763,764,765,766,767,770,771,776,777,778,779,780,781,782,783,784,785,786,787,788,789,790,791,794,795,797,798,799,800,801,802,803,804,806,807,808,809,810,811,812,813,814,815,816,817,818,819,820,821,822,823,824,825,826,827,828,829,830,831) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train SVM for Diagnosis and SVR for ADAS and Ventricles\n",
      "Evaluate predictions\n",
      "Diagnosis:\n",
      "mAUC = 0.625\n",
      "BAC = 0.598\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "print('Load data and select features')\n",
    "str_exp = '/Users/danielwu/Dropbox/Documents/CSCI5525/Project/'\n",
    "import os\n",
    "os.chdir(str_exp)\n",
    "\n",
    "tadpoleD1D2File = str_exp + 'TADPOLE_D1_D2.csv'\n",
    "\n",
    "Dtadpole = pd.read_csv(tadpoleD1D2File)\n",
    "\n",
    "# Create Diagnosis variable based on DXCHANGE\n",
    "idx_m = Dtadpole['PTGENDER'] == 'Male'\n",
    "Dtadpole.loc[idx_m, 'PTGENDER'] = 0\n",
    "idx_f = Dtadpole['PTGENDER'] == 'Female'\n",
    "Dtadpole.loc[idx_f, 'PTGENDER'] = 1\n",
    "\n",
    "idx_mci = Dtadpole['DXCHANGE'] == 4\n",
    "Dtadpole.loc[idx_mci, 'DXCHANGE'] = 2\n",
    "idx_ad = Dtadpole['DXCHANGE'] == 5\n",
    "Dtadpole.loc[idx_ad, 'DXCHANGE'] = 3\n",
    "idx_ad = Dtadpole['DXCHANGE'] == 6\n",
    "Dtadpole.loc[idx_ad, 'DXCHANGE'] = 3\n",
    "idx_cn = Dtadpole['DXCHANGE'] == 7\n",
    "Dtadpole.loc[idx_cn, 'DXCHANGE'] = 1\n",
    "idx_mci = Dtadpole['DXCHANGE'] == 8\n",
    "Dtadpole.loc[idx_mci, 'DXCHANGE'] = 2\n",
    "idx_cn = Dtadpole['DXCHANGE'] == 9\n",
    "Dtadpole.loc[idx_cn, 'DXCHANGE'] = 1\n",
    "Dtadpole = Dtadpole.rename(columns={'DXCHANGE': 'Diagnosis'})\n",
    "h = list(Dtadpole)\n",
    "\n",
    "D2 = Dtadpole['D2'].copy()\n",
    "\n",
    "# Select Leaderboard subjects\n",
    "tadpoleLB1LB2File = str_exp + 'TADPOLE_LB1_LB2.csv'\n",
    "LB_Table = pd.read_csv(tadpoleLB1LB2File)\n",
    "LB = LB_Table['LB1'] + LB_Table['LB2']\n",
    "idx_lb = LB.values >= 1\n",
    "Dtadpole = Dtadpole[idx_lb]\n",
    "\n",
    "# Select features\n",
    "cog_tests_attributes = [\"CDRSB\", 'EcogPtTotal',\n",
    "                        'MOCA', \"MMSE\", \"RAVLT_immediate\"]\n",
    "mri_measures = ['Hippocampus', 'WholeBrain',\n",
    "                'Entorhinal', 'MidTemp', 'Fusiform', 'ICV_bl']\n",
    "pet_measures = [\"FDG\", \"AV45\"]\n",
    "# csf_measures = [\"ABETA_UPENNBIOMK9_04_19_17\", \"TAU_UPENNBIOMK9_04_19_17\", \"PTAU_UPENNBIOMK9_04_19_17\"]\n",
    "risk_factors = [\"APOE4\", \"AGE\", \"PTGENDER\"]\n",
    "values = ['ADAS13', 'Ventricles', 'Diagnosis']\n",
    "\n",
    "Dtadpole = Dtadpole[['RID'] + values + mri_measures +\n",
    "                    pet_measures + cog_tests_attributes + risk_factors].copy()\n",
    "\n",
    "# Force values to numeric\n",
    "h = list(Dtadpole)\n",
    "\n",
    "for i in range(5, len(h)):\n",
    "\n",
    "    if Dtadpole[h[i]].dtype != 'float64':\n",
    "        Dtadpole[h[i]] = pd.to_numeric(Dtadpole[h[i]], errors='coerce')\n",
    "\n",
    "# Sort the dataframe based on age for each subject\n",
    "urid = np.unique(Dtadpole['RID'].values)\n",
    "Dtadpole_sorted = pd.DataFrame(columns=h)\n",
    "for i in range(len(urid)):\n",
    "\n",
    "    agei = Dtadpole.loc[Dtadpole['RID'] == urid[i], 'AGE']\n",
    "    idx_sortedi = np.argsort(agei)\n",
    "    D1 = Dtadpole.loc[idx_sortedi.index[idx_sortedi]]\n",
    "    ld = [Dtadpole_sorted, D1]\n",
    "    Dtadpole_sorted = pd.concat(ld)\n",
    "# Dtadpole_sorted = Dtadpole_sorted.drop(['AGE'], axis=1)\n",
    "\n",
    "# Save dataset\n",
    "Dtadpole_sorted.to_csv(\n",
    "    str_exp + 'IntermediateData/Leaderboard_NeuralNetBagging.csv', index=False)\n",
    "\n",
    "# Make list of RIDs in D2 to be predicted\n",
    "idx_lb2 = LB_Table['LB2'] == 1\n",
    "LB2_RID = LB_Table.loc[idx_lb2, 'RID']\n",
    "SLB2 = pd.Series(np.unique(LB2_RID.values))\n",
    "SLB2.to_csv(str_exp + '/IntermediateData/ToPredict.csv', index=False)\n",
    "\n",
    "# SVM for TADPOLE\n",
    "print('Train SVM for Diagnosis and SVR for ADAS and Ventricles')\n",
    "# Read Data\n",
    "str_in = os.path.join(str_exp, 'IntermediateData',\n",
    "                      'Leaderboard_NeuralNetBagging.csv')\n",
    "\n",
    "D = pd.read_csv(str_in)\n",
    "\n",
    "# Correct ventricle volume for ICV\n",
    "D = Dtadpole_sorted.copy()\n",
    "D['Ventricles_ICV'] = D['Ventricles'].values / D['ICV_bl'].values\n",
    "D['Hippocampus_ICV'] = D['Hippocampus'].values / D['ICV_bl'].values\n",
    "D['WholeBrain_ICV'] = D['WholeBrain'].values / D['ICV_bl'].values\n",
    "D['Entorhinal_ICV'] = D['Entorhinal'].values / D['ICV_bl'].values\n",
    "D['MidTemp_ICV'] = D['MidTemp'].values / D['ICV_bl'].values\n",
    "D['Fusiform_ICV'] = D['Fusiform'].values / D['ICV_bl'].values\n",
    "\n",
    "# Get Future Measurements for training prediction\n",
    "Y_FutureADAS13_temp = D['ADAS13'].copy()\n",
    "Y_FutureADAS13_temp[:] = np.nan\n",
    "Y_FutureVentricles_ICV_temp = D['Ventricles_ICV'].copy()\n",
    "Y_FutureVentricles_ICV_temp[:] = np.nan\n",
    "Y_FutureDiagnosis_temp = D['Diagnosis'].copy()\n",
    "Y_FutureDiagnosis_temp[:] = np.nan\n",
    "RID = D['RID'].copy()\n",
    "uRIDs = np.unique(RID)\n",
    "for i in range(len(uRIDs)):\n",
    "    idx = RID == uRIDs[i]\n",
    "    idx_copy = np.copy(idx)\n",
    "    idx_copy[np.where(idx)[-1][-1]] = False\n",
    "    Y_FutureADAS13_temp[idx_copy] = D.loc[idx, 'ADAS13'].values[1:]\n",
    "    Y_FutureVentricles_ICV_temp[idx_copy] = D.loc[idx,\n",
    "                                                  'Ventricles_ICV'].values[1:]\n",
    "    Y_FutureDiagnosis_temp[idx_copy] = D.loc[idx, 'Diagnosis'].values[1:]\n",
    "Dtrain = D.drop(['RID', 'Diagnosis', 'Ventricles', 'Hippocampus',\n",
    "                 'WholeBrain', 'Entorhinal', 'MidTemp', 'Fusiform'], axis=1).copy()\n",
    "\n",
    "Dtrainmat = Dtrain.as_matrix()\n",
    "Dtrainmat = Dtrainmat.astype(float)\n",
    "h = list(Dtrain)\n",
    "\n",
    "m = []\n",
    "s = []\n",
    "\n",
    "for i in range(Dtrainmat.shape[1]):\n",
    "    m.append(np.nanmean(Dtrainmat[:, i]))\n",
    "    s.append(np.nanstd(Dtrainmat[:, i]))\n",
    "    Dtrainmat[np.isnan(Dtrainmat[:, i]), i] = m[i]\n",
    "    Dtrainmat[:, i] = (Dtrainmat[:, i] - m[i]) / s[i]\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "str_exp = '/Users/danielwu/Dropbox/Documents/CSCI5525/Project/'\n",
    "S = pd.read_csv(os.path.join(str_exp, 'IntermediateData',\n",
    "                             'ToPredict.csv'), header=None)\n",
    "S = S.values\n",
    "\n",
    "Dtestmat = np.zeros((len(S), Dtrainmat.shape[1]))\n",
    "# y_Dtestmat = np.zeros((len(S),1))\n",
    "for i in range(len(S)):\n",
    "    idx_S = RID.values == S[i]\n",
    "    Dtestmat[i, :] = Dtrainmat[np.where(idx_S)[0][-1], :]\n",
    "#     y_Dtestmat[i] = Dtrainmat[np.where(idx_S)[0][-1], :]\n",
    "\n",
    "# mt = []\n",
    "# st = []\n",
    "# for i in range(Dtrainmat.shape[1]):\n",
    "#     mt.append(np.nanmean(Dtestmat[:, i]))\n",
    "#     st.append(np.nanstd(Dtestmat[:, i]))\n",
    "#     Dtestmat[np.isnan(Dtestmat[:, i]), i] = mt[i]\n",
    "#     Dtestmat[:, i] = (Dtestmat[:, i] - mt[i]) / st[i]\n",
    "\n",
    "    \n",
    "p = model.predict(Dtestmat)\n",
    "\n",
    "# Write ouput format to files\n",
    "o = np.column_stack((S, S, S, p))\n",
    "count = 0\n",
    "years = [str(a) for a in range(2010, 2018)]\n",
    "months = [str(a).zfill(2) for a in range(1, 13)]\n",
    "ym = [y + '-' + mo for y in years for mo in months]\n",
    "ym = ym[4:-8]\n",
    "nr_pred = len(ym)\n",
    "o1 = np.zeros((o.shape[0] * nr_pred, o.shape[1]))\n",
    "ym1 = [a for b in range(0, len(S)) for a in ym]\n",
    "for i in range(len(o)):\n",
    "    o1[count:count + nr_pred] = o[i]\n",
    "    o1[count:count + nr_pred, 1] = range(1, nr_pred + 1)\n",
    "    count = count + nr_pred\n",
    "\n",
    "output = pd.DataFrame(o1, columns=['RID', 'Forecast Month', 'Forecast Date','CN relative probability', 'MCI relative probability', 'AD relative probability'])\n",
    "output['Forecast Month'] = output['Forecast Month'].astype(int)\n",
    "output['Forecast Date'] = ym1\n",
    "\n",
    "str_out_final = os.path.join(str_exp, 'IntermediateData', 'TADPOLE_Submission_Leaderboard_NeuralNet.csv')\n",
    "output.to_csv(str_out_final, header=True, index=False)\n",
    "\n",
    "print('Evaluate predictions')\n",
    "R = pd.read_csv('./TADPOLE_LB4.csv')\n",
    "import evalOneSubmission as eos\n",
    "mAUC, bca = eos.evalOneSub(R, output)\n",
    "\n",
    "print('Diagnosis:')\n",
    "print('mAUC = ' + \"%0.3f\" % mAUC)\n",
    "print('BAC = ' + \"%0.3f\" % bca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BaggingAverage(results, verbose = False):\n",
    "    \"\"\"Average the probabilities for a given test sample across models to finalize the prediction\"\"\" \n",
    "    correct = 0.0\n",
    "    missedTests = []\n",
    "    predictionMisses = []\n",
    "    for i in range(samples):\n",
    "        gs = np.argmax(y_test[i])\n",
    "        ra = np.zeros(3)\n",
    "        for r in reuslts:\n",
    "            ra = += r[i]\n",
    "        f = ra/ len(results)\n",
    "        prd = np.argmax(f)\n",
    "        frnd = np.round(f,5)\n",
    "    if(prd == gs):\n",
    "        correct += 1\n",
    "    else:\n",
    "        if(verbose):\n",
    "            for r in results:\n",
    "                print(np.round(r[i], 5))\n",
    "            print(\"Agg: \" + str(frnd))\n",
    "            print(\"Predicted: \" + str(prd))\n",
    "            print(\"GoldStd:   \" + str(gs))\n",
    "        missedTests.append(X_test[i])\n",
    "        predictionMisses.append(prd)\n",
    "    \n",
    "    return correct\n",
    "\n",
    "def BaggingVote(results, verbose = False):\n",
    "    \"\"\"Vote based on the probabilities for a given test sample across models to finalize the prediction\"\"\"\n",
    "    correct = 0.0\n",
    "    \n",
    "    missedTests = []\n",
    "    predictionMisses = []\n",
    "    for i in range(samples):\n",
    "        #print(y_test[i])\n",
    "        gs = np.argmax(y_test[i])\n",
    "        votes = np.zeros(3)\n",
    "        for r in results:\n",
    "            # Use argmax to get the primary vote from this model and increment the cooresponding slot \n",
    "            # in the aggregated result\n",
    "            vote = np.argmax(r[i])\n",
    "            #print(vote)\n",
    "            votes[vote] += 1\n",
    "        # Use argmax again to extract the slot that has the most votes.\n",
    "        prd = np.argmax(votes)\n",
    "\n",
    "        if(prd == gs):\n",
    "            correct += 1\n",
    "        else:\n",
    "            if(verbose):\n",
    "                for r in results:\n",
    "                    print(np.round(r[i], 5))\n",
    "                print(\"Votes: \" + str(votes))\n",
    "                print(\"Predicted: \" + str(prd))\n",
    "                print(\"GoldStd:   \" + str(gs))\n",
    "            missedTests.append(X_test[i])\n",
    "            predictionMisses.append(prd)\n",
    "    \n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write ouput format to files\n",
    "o=np.column_stack((S,S,S,p))\n",
    "count=0\n",
    "years=[str(a) for a in range(2010,2018)]\n",
    "months=[str(a).zfill(2) for a in range(1,13)]\n",
    "ym=[y + '-' + mo for y in years for mo in months ]\n",
    "ym=ym[4:-8]\n",
    "nr_pred=len(ym)\n",
    "o1 = np.zeros((o.shape[0]*nr_pred,o.shape[1]))\n",
    "ym1 = [a for b in range(0, len(S)) for a in ym ]\n",
    "for i in range(len(o)):\n",
    "    o1[count:count+nr_pred]=o[i]\n",
    "    o1[count:count+nr_pred,1]=range(1,nr_pred+1)\n",
    "    count=count+nr_pred\n",
    "    \n",
    "\n",
    "output=pd.DataFrame(o1, columns=['RID','Forecast Month','Forecast Date','CN relative probability','MCI relative probability','AD relative probability'])\n",
    "output['Forecast Month'] = output['Forecast Month'].astype(int)\n",
    "output['Forecast Date'] = ym1\n",
    "\n",
    "str_out_final = os.path.join(str_exp, 'IntermediateData','TADPOLE_Submission_Leaderboard_BenchmarkSVM.csv')\n",
    "output.to_csv(str_out_final,header=True,index=False)\n",
    "\n",
    "\n",
    "print('Evaluate predictions')\n",
    "R=pd.read_csv('./TADPOLE_LB4.csv')\n",
    "import evalOneSubmission as eos\n",
    "mAUC, bca = eos.evalOneSub(R,output)\n",
    "\n",
    "print('Diagnosis:')\n",
    "print('mAUC = ' + \"%0.3f\" % mAUC,)\n",
    "print ('BAC = ' + \"%0.3f\" % bca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is our percentage success from averaging?\n",
    "print (\"Bagging w/ Averaging\")\n",
    "print (\"--------------------\")\n",
    "correct = BaggingAverage(results)\n",
    "print (\"Misses: \" + str(samples - correct))\n",
    "print correct/samples\n",
    "\n",
    "# Separator...\n",
    "print\n",
    "print\n",
    "\n",
    "# What is our percentage success from voting?\n",
    "print (\"Bagging w/ Voting\")\n",
    "print (\"-----------------\")\n",
    "correctVote = BaggingVote(results)\n",
    "print (\"Misses: \" + str(samples - correctVote))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
